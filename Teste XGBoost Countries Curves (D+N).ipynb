{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor, plot_importance, plot_tree\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import datetime as dt\n",
    "import joblib\n",
    "\n",
    "%matplotlib qt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mov_avg_std(df, col, N):\n",
    "    \"\"\"\n",
    "    Given a dataframe, get mean and std dev at timestep t using values from t-1, t-2, ..., t-N.\n",
    "    Inputs\n",
    "        df         : dataframe. Can be of any length.\n",
    "        col        : name of the column you want to calculate mean and std dev\n",
    "        N          : get mean and std dev at timestep t using values from t-1, t-2, ..., t-N\n",
    "    Outputs\n",
    "        df_out     : same as df but with additional column containing mean and std dev\n",
    "    \"\"\"\n",
    "\n",
    "    mean_list = df[col].rolling(window = N, min_periods=1).mean() # len(mean_list) = len(df)\n",
    "    std_list = df[col].rolling(window = N, min_periods=1).std()   # first value will be NaN, because normalized by N-1\n",
    "    \n",
    "    # Add one timestep to the predictions\n",
    "    mean_list = np.concatenate((np.array([np.nan]), np.array(mean_list[:-1])))\n",
    "    std_list = np.concatenate((np.array([np.nan]), np.array(std_list[:-1])))\n",
    "    \n",
    "    # Append mean_list to df\n",
    "    df_out = df.copy()\n",
    "    df_out[col + '_mean'] = mean_list\n",
    "    df_out[col + '_std'] = std_list\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "def scale_row(row, feat_mean, feat_std):\n",
    "    \"\"\"\n",
    "    Given a pandas series in row, scale it to have 0 mean and var 1 using feat_mean and feat_std\n",
    "    Inputs\n",
    "        row      : pandas series. Need to scale this.\n",
    "        feat_mean: mean  \n",
    "        feat_std : standard deviation\n",
    "    Outputs\n",
    "        row_scaled : pandas series with same length as row, but scaled\n",
    "    \"\"\"\n",
    "    # If feat_std = 0 (this happens if adj_close doesn't change over N days), \n",
    "    # set it to a small number to avoid division by zero\n",
    "    feat_std = 0.001 if feat_std == 0 else feat_std\n",
    "    \n",
    "    row_scaled = (row-feat_mean) / feat_std\n",
    "    \n",
    "    return row_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mov_avg_std(df, col, N):\n",
    "    \"\"\"\n",
    "    Given a dataframe, get mean and std dev at timestep t using values from t-1, t-2, ..., t-N.\n",
    "    Inputs\n",
    "        df         : dataframe. Can be of any length.\n",
    "        col        : name of the column you want to calculate mean and std dev\n",
    "        N          : get mean and std dev at timestep t using values from t-1, t-2, ..., t-N\n",
    "    Outputs\n",
    "        df_out     : same as df but with additional column containing mean and std dev\n",
    "    \"\"\"\n",
    "    mean_list = df[col].rolling(window = N, min_periods=1).mean() # len(mean_list) = len(df)\n",
    "    std_list = df[col].rolling(window = N, min_periods=1).std()   # first value will be NaN, because normalized by N-1\n",
    "    \n",
    "    # Add one timestep to the predictions\n",
    "    mean_list = np.concatenate((np.array([np.nan]), np.array(mean_list[:-1])))\n",
    "    std_list = np.concatenate((np.array([np.nan]), np.array(std_list[:-1])))\n",
    "    \n",
    "    # Append mean_list to df\n",
    "    df_out = df.copy()\n",
    "    df_out[col + '_mean'] = mean_list\n",
    "    df_out[col + '_std'] = std_list\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "def do_scaling(df, N, cols_list):\n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    for col in tqdm_notebook(cols_list):\n",
    "        feat_list = [col + '_lag_' + str(shift) for shift in range(1, N+1)]\n",
    "        save_list = [col + '_scaled_lag_' + str(shift) for shift in range(1, N+1)]\n",
    "        temp = df.apply(lambda row: scale_row(row[feat_list], row[col+'_mean'], row[col+'_std']), axis=1)\n",
    "        temp.rename(columns={feat_list[i]:save_list[i] for i in range(len(save_list))}, inplace=True)\n",
    "        \n",
    "        df_scaled = pd.concat([df_scaled, temp], axis=1)\n",
    "\n",
    "    return df_scaled\n",
    "\n",
    "def pred_xgboost(model, N, H, prev_vals, prev_mean_val, prev_std_val, lag_cols):\n",
    "    \"\"\"\n",
    "    Do recursive forecasting using xgboost\n",
    "    Inputs\n",
    "        model              : the xgboost model\n",
    "        N                  : for feature at day t, we use lags from t-1, t-2, ..., t-N as features\n",
    "        H                  : forecast horizon\n",
    "        prev_vals          : numpy array. If predict at time t, \n",
    "                             prev_vals will contain the N unscaled values at t-1, t-2, ..., t-N\n",
    "        prev_mean_val      : the mean of the unscaled values at t-1, t-2, ..., t-N\n",
    "        prev_std_val       : the std deviation of the unscaled values at t-1, t-2, ..., t-N\n",
    "    Outputs\n",
    "        Times series of predictions. Numpy array of shape (H,). This is unscaled.\n",
    "    \"\"\"\n",
    "    forecast = prev_vals.copy()\n",
    "    \n",
    "    for n in range(H):\n",
    "        filter_date = forecast['Date'].max() - dt.timedelta(days=N)\n",
    "        forecast = forecast[forecast['Date'] >= filter_date]\n",
    "        \n",
    "             \n",
    "        # Do prediction\n",
    "        est_scaled = model.predict(forecast)\n",
    "        \n",
    "        # Precisa fazer: \n",
    "        # - Combinar resultado da predição com os dados para proxima predição\n",
    "        # - Coletar informações adicionais e rodar novamente lag cols \n",
    "        # - Criar um array de resposta \n",
    "\n",
    "           \n",
    "    return forecast[-H:]\n",
    "\n",
    "def train_pred_eval_model(X_train_scaled,\n",
    "                          y_train_scaled,\n",
    "                          y_test,\n",
    "                          N,\n",
    "                          H,\n",
    "                          prev_vals,\n",
    "                          prev_mean_val,\n",
    "                          prev_std_val,\n",
    "                          lag_cols,\n",
    "                          seed=100,\n",
    "                          n_estimators=100,\n",
    "                          max_depth=3,\n",
    "                          learning_rate=0.1,\n",
    "                          min_child_weight=1,\n",
    "                          subsample=1,\n",
    "                          colsample_bytree=1,\n",
    "                          colsample_bylevel=1,\n",
    "                          gamma=0):\n",
    "    '''\n",
    "    Train model, do prediction, scale back to original range and do evaluation\n",
    "    Use XGBoost here.\n",
    "    Inputs\n",
    "        X_train_scaled     : features for training. Scaled to have mean 0 and variance 1\n",
    "        y_train_scaled     : target for training. Scaled to have mean 0 and variance 1\n",
    "        y_test             : target for test. Actual values, not scaled.\n",
    "        N                  : for feature at day t, we use lags from t-1, t-2, ..., t-N as features\n",
    "        H                  : forecast horizon\n",
    "        prev_vals          : numpy array. If scaled[0] is at time t, prev_vals will contain the N-1 unscaled values at t-1, t-2, ...\n",
    "        prev_mean_val      : the mean of the unscaled values at t-1, t-2, ..., t-N\n",
    "        prev_std_val       : the std deviation of the unscaled values at t-1, t-2, ..., t-N\n",
    "        seed               : model seed\n",
    "        n_estimators       : number of boosted trees to fit\n",
    "        max_depth          : maximum tree depth for base learners\n",
    "        learning_rate      : boosting learning rate (xgb’s “eta”)\n",
    "        min_child_weight   : minimum sum of instance weight(hessian) needed in a child\n",
    "        subsample          : subsample ratio of the training instance\n",
    "        colsample_bytree   : subsample ratio of columns when constructing each tree\n",
    "        colsample_bylevel  : subsample ratio of columns for each split, in each level\n",
    "        gamma              : \n",
    "    Outputs\n",
    "        rmse               : root mean square error of y_test and est\n",
    "        mape               : mean absolute percentage error of y_test and est\n",
    "        mae                : mean absolute error of y_test and est\n",
    "        est                : predicted values. Same length as y_test\n",
    "    '''\n",
    "\n",
    "    model = XGBRegressor(objective ='reg:squarederror',\n",
    "                         seed=seed,\n",
    "                         n_estimators=n_estimators,\n",
    "                         max_depth=max_depth,\n",
    "                         learning_rate=learning_rate,\n",
    "                         min_child_weight=min_child_weight,\n",
    "                         subsample=subsample,\n",
    "                         colsample_bytree=colsample_bytree,\n",
    "                         colsample_bylevel=colsample_bylevel,\n",
    "                         gamma=gamma)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train_scaled)\n",
    "    \n",
    "    # Get predicted labels and scale back to original range\n",
    "    est = pred_xgboost(model, N, H, prev_vals, prev_mean_val, prev_std_val, lag_cols)\n",
    "\n",
    "    # Calculate RMSE, MAPE, MAE\n",
    "#     rmse = get_rmse(y_test, est)\n",
    "#     mape = get_mape(y_test, est)\n",
    "#     mae = get_mae(y_test, est)\n",
    "    \n",
    "#     return rmse, mape, mae, est, model.feature_importances_\n",
    "\n",
    "def add_lags(df, N, lag_cols):\n",
    "    \"\"\"\n",
    "    Add lags up to N number of days to use as features\n",
    "    The lag columns are labelled as 'adj_close_lag_1', 'adj_close_lag_2', ... etc.\n",
    "    \"\"\"\n",
    "    # Use lags up to N number of days to use as features\n",
    "    df_w_lags = df.copy()\n",
    "    \n",
    "    for country in df_w_lags['Country'].unique():\n",
    "        df_w_lags.loc[df_w_lags['Country'] == country, 'order_day'] = [x for x in list(range(len(df_w_lags.loc[df_w_lags['Country'] == country, :])))]\n",
    "\n",
    "    # merging_keys\n",
    "    merging_keys = ['Country', 'country_code','order_day']\n",
    "    \n",
    "    shift_range = [x+1 for x in range(N)]\n",
    "    \n",
    "    for shift in tqdm_notebook(shift_range):\n",
    "        train_shift = df_w_lags[merging_keys + lag_cols].copy()\n",
    "    \n",
    "        for country in df_w_lags['Country'].unique():    \n",
    "            # E.g. order_day of 0 becomes 1, for shift = 1.\n",
    "            # So when this is merged with order_day of 1 in df_confirmed, this will represent lag of 1.\n",
    "\n",
    "            train_shift.loc[train_shift['Country'] == country, 'order_day'] =train_shift.loc[train_shift['Country'] == country, 'order_day'] + shift\n",
    "\n",
    "        foo = lambda x: '{}_lag_{}'.format(x, shift) if x in lag_cols else x\n",
    "        train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "        df_w_lags = pd.merge(df_w_lags, train_shift, on=merging_keys, how='left')\n",
    "    \n",
    "    del train_shift\n",
    "    \n",
    "    return df_w_lags\n",
    "\n",
    "def get_error_metrics(df,\n",
    "                      val_date,\n",
    "                      N,\n",
    "                      H,\n",
    "                      lag_cols,\n",
    "                      target_col,\n",
    "                      seed=100,\n",
    "                      n_estimators=100,\n",
    "                      max_depth=3,\n",
    "                      learning_rate=0.1,\n",
    "                      min_child_weight=1,\n",
    "                      subsample=1,\n",
    "                      colsample_bytree=1,\n",
    "                      colsample_bylevel=1,\n",
    "                      gamma=0):\n",
    "    \"\"\"\n",
    "    Given a series consisting of both train+validation, do predictions of forecast horizon H on the validation set, \n",
    "    at H/2 intervals.\n",
    "    Inputs\n",
    "        df                 : train + val dataframe. len(df) = train_size + val_size\n",
    "        train_size         : size of train set\n",
    "        N                  : for feature at day t, we use lags from t-1, t-2, ..., t-N as features\n",
    "        H                  : forecast horizon\n",
    "        seed               : model seed\n",
    "        n_estimators       : number of boosted trees to fit\n",
    "        max_depth          : maximum tree depth for base learners\n",
    "        learning_rate      : boosting learning rate (xgb’s “eta”)\n",
    "        min_child_weight   : minimum sum of instance weight(hessian) needed in a child\n",
    "        subsample          : subsample ratio of the training instance\n",
    "        colsample_bytree   : subsample ratio of columns when constructing each tree\n",
    "        colsample_bylevel  : subsample ratio of columns for each split, in each level\n",
    "        gamma              : \n",
    "\n",
    "    Outputs\n",
    "        mean of rmse, mean of mape, mean of mae, dictionary of predictions\n",
    "    \"\"\"\n",
    "    rmse_list = [] # root mean square error\n",
    "    mape_list = [] # mean absolute percentage error\n",
    "    mae_list = []  # mean absolute error\n",
    "    preds_dict = {}\n",
    "    \n",
    "    original_cols = df.columns.values\n",
    "    # Add lags up to N number of days to use as features\n",
    "    df = add_lags(df, N, lag_cols)\n",
    "    \n",
    "    # Get mean and std dev at timestamp t using values from t-1, ..., t-N\n",
    "    for col in lag_cols:\n",
    "        df = get_mov_avg_std(df, col, N)\n",
    "    \n",
    "    # Do scaling\n",
    "    df = do_scaling(df, N, lag_cols)\n",
    "        \n",
    "    # Get list of features\n",
    "    features =  ['country_code']\n",
    "    for n in range(N,0,-1):\n",
    "        for col in lag_cols:\n",
    "            features.append(col+\"_scaled_lag_\"+str(n))\n",
    "    \n",
    "    \n",
    "    # Split into train and test\n",
    "    train = df[df['Date'] < val_date].copy()\n",
    "    test = df[df['Date'] >= val_date].copy()\n",
    "    \n",
    "    train.sort_values(by=['Country', 'Date'], inplace=True)\n",
    "    test.sort_values(by=['Country', 'Date'], inplace=True)\n",
    "        \n",
    "    # Split into X and y\n",
    "    X_train_scaled = train[features]\n",
    "    y_train_scaled = train[target_col]\n",
    "    y_test = test[target_col]\n",
    "    mask_prev_vals = (train['Date'] >= (val_date - dt.timedelta(days=N)))\n",
    "    prev_vals = (train.loc[mask_prev_vals, features])\n",
    "    prev_mean_val = test.iloc[0][target_col+'_mean']\n",
    "    prev_std_val = test.iloc[0][target_col+'_std']\n",
    "    \n",
    "    print(\"X_train_scaled.shape:\", X_train_scaled.shape)\n",
    "    print(\"y_train_scaled.shape\", y_train_scaled.shape)\n",
    "    print(\"y_test_scaled.shape:\", y_test.shape)\n",
    "    print(\"prev_vals.shape:\", prev_vals.shape)\n",
    "    print(prev_mean_val)\n",
    "    print(prev_std_val)\n",
    "    \n",
    "    # Train and predict D+N\n",
    "    rmse, mape, mae, est, _ = train_pred_eval_model(X_train_scaled,\n",
    "                                                    y_train_scaled,\n",
    "                                                    y_test,\n",
    "                                                    N,\n",
    "                                                    H,\n",
    "                                                    prev_vals,\n",
    "                                                    prev_mean_val,\n",
    "                                                    prev_std_val,\n",
    "                                                    lag_cols,\n",
    "                                                    seed=seed,\n",
    "                                                    n_estimators=n_estimators,\n",
    "                                                    max_depth=max_depth,\n",
    "                                                    learning_rate=learning_rate,\n",
    "                                                    min_child_weight=min_child_weight,\n",
    "                                                    subsample=subsample,\n",
    "                                                    colsample_bytree=colsample_bytree,\n",
    "                                                    colsample_bylevel=colsample_bylevel,\n",
    "                                                    gamma=gamma)\n",
    "    \n",
    "    return rmse, mape, mae, est "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid = pd.read_csv(\"./input/covid_19_clear.csv\", parse_dates=['Date'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_covid[df_covid['Country'] == 'Brazil'].plot(x='Date', y='Confirmed', style='b-', grid=True)\n",
    "ax.set_xlabel(\"date\")\n",
    "ax.set_ylabel(\"Confirmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = pd.read_csv(\"./input/country_info.csv\", parse_dates=['Lockdown Start Date'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_covid, df_info, left_on='Country', right_on='Country', how='inner', suffixes=('', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_covid\n",
    "del df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Date'].dt.month == 1, 'Country Temperature ºC'] = df['Temperature Jan (ºC)']\n",
    "df.loc[df['Date'].dt.month == 2, 'Country Temperature ºC'] = df['Temperature Feb (ºC)']\n",
    "df.loc[df['Date'].dt.month == 3, 'Country Temperature ºC'] = df['Temperature Mar (ºC)']\n",
    "df.loc[df['Date'].dt.month == 4, 'Country Temperature ºC'] = df['Temperature Apr (ºC)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cases = 50\n",
    "\n",
    "df = df[df['Confirmed'] > n_cases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "df[\"country_code\"] = lb_make.fit_transform(df[\"Country\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['Country','Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separação dos conjuntos de treinamento para casos confirmados e mortes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confirmed = df.drop(columns=['Temperature Jan (ºC)', 'Temperature Feb (ºC)', 'Temperature Mar (ºC)', 'Temperature Apr (ºC)', 'Deaths', 'Recovered', 'Daily new confirmed deaths due to COVID-19 (rolling 3-day average)', 'Hospital beds (per 1,000 people)', 'Lockdown Start Date'])\n",
    "df_deaths = df.drop(columns=['Temperature Jan (ºC)', 'Temperature Feb (ºC)', 'Temperature Mar (ºC)', 'Temperature Apr (ºC)', 'Recovered', 'Lockdown Start Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento e Predição"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determina o numero de dias que quer prever (horizonte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 10\n",
    "\n",
    "valid_date = df_confirmed.Date.max() - dt.timedelta(days=H)\n",
    "\n",
    "print(\"Predicting on date %s, with forecast horizon H = %d\" % (valid_date, H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treina e prevê os valores futuros para confirmados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4 # Numeros de dias para usar como lag\n",
    "\n",
    "lag_cols = df_confirmed.columns.values.tolist()[2:22] # Colunas para gerar lag\n",
    "target_col = 'Confirmed'\n",
    "\n",
    "get_error_metrics(df_confirmed, valid_date, N, H, lag_cols, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
